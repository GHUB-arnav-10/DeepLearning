{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYHIAXdA50VOuKa6bMhtDc"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvK8UiYLDuML",
        "outputId": "ca57eb63-ea9f-4405-d9fa-3257126b8afa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/1000] Loss: 40.31522452831268\n",
            "Epoch [200/1000] Loss: 39.80284970998764\n",
            "Epoch [300/1000] Loss: 39.412530303001404\n",
            "Epoch [400/1000] Loss: 39.537276923656464\n",
            "Epoch [500/1000] Loss: 38.93186718225479\n",
            "Epoch [600/1000] Loss: 38.874234080314636\n",
            "Epoch [700/1000] Loss: 39.381654143333435\n",
            "Epoch [800/1000] Loss: 39.318923592567444\n",
            "Epoch [900/1000] Loss: 39.07107764482498\n",
            "Epoch [1000/1000] Loss: 39.16583997011185\n",
            "Words similar to 'like': ['like', 'arnav', 'and', 'i', 'my']\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "text = \"Hey my name is Arnav and i like NLP.\"\n",
        "\n",
        "\n",
        "text = text.lower().split()\n",
        "\n",
        "\n",
        "vocab = set(text)\n",
        "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "\n",
        "\n",
        "embedding_dim = 100\n",
        "window_size = 2\n",
        "learning_rate = 0.001\n",
        "epochs = 1000\n",
        "\n",
        "\n",
        "data = []\n",
        "for i in range(len(text)):\n",
        "    target_word = text[i]\n",
        "    context_words = [text[j] for j in range(i - window_size, i + window_size + 1) if i != j and 0 <= j < len(text)]\n",
        "    for context_word in context_words:\n",
        "        data.append((word_to_idx[target_word], word_to_idx[context_word]))\n",
        "\n",
        "\n",
        "class SkipGram(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(SkipGram, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, target, context):\n",
        "        target_embeds = self.embeddings(target)\n",
        "        context_scores = self.linear(target_embeds)\n",
        "        return context_scores\n",
        "\n",
        "\n",
        "model = SkipGram(len(vocab), embedding_dim)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    np.random.shuffle(data)\n",
        "    for target_word, context_word in data:\n",
        "        optimizer.zero_grad()\n",
        "        target_word = torch.tensor(target_word, dtype=torch.long)\n",
        "        context_word = torch.tensor(context_word, dtype=torch.long)\n",
        "        context_scores = model(target_word, context_word)\n",
        "        loss = F.cross_entropy(context_scores, context_word)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{epochs}] Loss: {total_loss}')\n",
        "\n",
        "\n",
        "word_embeddings = model.embeddings.weight.detach().numpy()\n",
        "\n",
        "\n",
        "def find_similar_words(embedding_matrix, target_word, top_n=5):\n",
        "    target_idx = word_to_idx.get(target_word)\n",
        "    if target_idx is None:\n",
        "        print(f\"'{target_word}' not found in vocabulary.\")\n",
        "        return\n",
        "\n",
        "    target_embedding = embedding_matrix[target_idx]\n",
        "    similarities = np.dot(embedding_matrix, target_embedding)\n",
        "\n",
        "\n",
        "    most_similar_indices = np.argsort(similarities)[::-1][:top_n]\n",
        "    most_similar_words = [idx_to_word[idx] for idx in most_similar_indices]\n",
        "\n",
        "    return most_similar_words\n",
        "\n",
        "\n",
        "target_word = \"like\"\n",
        "similar_words = find_similar_words(word_embeddings, target_word)\n",
        "print(f\"Words similar to '{target_word}': {similar_words}\")\n",
        "\n",
        "\n"
      ]
    }
  ]
}